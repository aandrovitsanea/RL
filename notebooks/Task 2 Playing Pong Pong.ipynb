{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182ed633",
   "metadata": {},
   "source": [
    "# Step-by-Step Guide to Implementing DQN for Pong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6d4b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actual environment location may have moved due to redirects, links or junctions.\n",
      "  Requested location: \"Z:\\Desktops\\BW\\Anna.Androvitsanea\\Desktop\\misc\\RL\\rl-env\\Scripts\\python.exe\"\n",
      "  Actual location:    \"\\\\fileserver05.intern.rossmann.de\\Desktops_BW$\\Anna.Androvitsanea\\Desktop\\misc\\RL\\rl-env\\Scripts\\python.exe\"\n"
     ]
    }
   ],
   "source": [
    "! python -m venv rl-env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d464f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rl-env\\Scripts\\activate  # FÃ¼r Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow==2.3.0 keras rl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9b5cc",
   "metadata": {},
   "source": [
    "# Using tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527a045",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the Environment\n",
    "First, ensure you have all necessary libraries installed. If not, you can install them using pip:\n",
    "\n",
    "```bash\n",
    "pip install gym[atari] gym[accept-rom-license] keras-rl2 tensorflow opencv-python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfae50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl2 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (1.24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: autorom~=0.4.2 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\anna.androvitsanea\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\anna.androvitsanea\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.2)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.1.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.65.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\anna.androvitsanea\\appdata\\roaming\\python\\python310\\site-packages (from click->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gym[atari] gym[accept-rom-license] keras-rl2 tensorflow opencv-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8bbc0",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Initialize Environment\n",
    "In this step, we will import the necessary libraries and initialize the Pong environment using OpenAI Gym. We will also retrieve the number of possible actions in the Pong game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56427361",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_from_config' from 'tensorflow.keras.models' (C:\\Users\\Anna.Androvitsanea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\_tf_keras\\keras\\models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation, Flatten, Conv2D, Permute\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearAnnealedPolicy, EpsGreedyQPolicy\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialMemory\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent, NAFAgent, ContinuousDQNAgent\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mddpg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDPGAgent\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CEMAgent\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\agents\\dqn.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lambda, Input, Layer, Dense\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EpsGreedyQPolicy, GreedyQPolicy\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_q\u001b[39m(y_true, y_pred):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\policy.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPolicy\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract base class for all implemented policies.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Each policy helps with selection of action to take on an environment.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        agent (rl.core.Agent): Agent used\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rl\\util.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_config, Sequential, Model, model_from_config\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptimizers\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'model_from_config' from 'tensorflow.keras.models' (C:\\Users\\Anna.Androvitsanea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\_tf_keras\\keras\\models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, Permute\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Set the Atari environment for Pong.\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "\n",
    "# Getting the number of actions in Pong (the number of possible moves)\n",
    "nb_actions = env.action_space.n\n",
    "print(f\"Number of actions: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9dbe1e",
   "metadata": {},
   "source": [
    "## Step 3: Build the Neural Network (CNN) Model\n",
    "In this step, we will define the neural network model using Keras. The model will consist of convolutional layers followed by dense layers to predict Q-values for each action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05b4326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anna.Androvitsanea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\reshaping\\permute.py:36: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Computed output size would be negative. Received `inputs shape=(None, 20, 0, 32)`, `kernel shape=(4, 4, 32, 64)`, `dilation_rate=[1 1]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Permute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m), input_shape\u001b[38;5;241m=\u001b[39minput_shape))  \u001b[38;5;66;03m# Convert from (width, height, channels) to (channels, width, height)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\sequential.py:120\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer, rebuild)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\sequential.py:139\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    138\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\layer.py:223\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(original_build_method)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m--> 223\u001b[0m         original_build_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\sequential.py:183\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 183\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\ops\\operation_utils.py:221\u001b[0m, in \u001b[0;36mcompute_conv_output_shape\u001b[1;34m(input_shape, filters, kernel_size, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_spatial_shape)):\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m none_dims \u001b[38;5;129;01mand\u001b[39;00m output_spatial_shape[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputed output size would be negative. Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`inputs shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`kernel shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkernel_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dilation_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdilation_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m             )\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m padding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m padding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    228\u001b[0m     output_spatial_shape \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor((spatial_shape \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m strides) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Computed output size would be negative. Received `inputs shape=(None, 20, 0, 32)`, `kernel shape=(4, 4, 32, 64)`, `dilation_rate=[1 1]`."
     ]
    }
   ],
   "source": [
    "input_shape = (84, 84, 4)  # We will use grayscale images of size 84x84 and stack 4 frames together\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))  # Convert from (width, height, channels) to (channels, width, height)\n",
    "model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu'))\n",
    "model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(nb_actions, activation='linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec0db9",
   "metadata": {},
   "source": [
    "## Step 4: Set Up the DQN Agent\n",
    "We will configure the DQN agent with the neural network model, memory for experience replay, and the epsilon-greedy policy for exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6c08ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SequentialMemory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Configuring the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[43mSequentialMemory\u001b[49m(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, window_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      3\u001b[0m policy \u001b[38;5;241m=\u001b[39m LinearAnnealedPolicy(EpsGreedyQPolicy(), attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m, value_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, value_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, value_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.05\u001b[39m, nb_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m)\n\u001b[0;32m      5\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQNAgent(model\u001b[38;5;241m=\u001b[39mmodel, nb_actions\u001b[38;5;241m=\u001b[39mnb_actions, memory\u001b[38;5;241m=\u001b[39mmemory, nb_steps_warmup\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m, target_model_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, policy\u001b[38;5;241m=\u001b[39mpolicy, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.99\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SequentialMemory' is not defined"
     ]
    }
   ],
   "source": [
    "# Configuring the agent\n",
    "memory = SequentialMemory(limit=1000000, window_length=4)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=0.1, value_test=.05, nb_steps=1000000)\n",
    "\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=50000, target_model_update=10000, policy=policy, gamma=.99)\n",
    "dqn.compile(Adam(lr=0.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6ced6",
   "metadata": {},
   "source": [
    "## Step 5: Preprocess the Input\n",
    "To play Pong, we need to preprocess the input frames by converting them to grayscale, resizing, and stacking consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5804a17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Applying the preprocessing wrapper\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m env \u001b[38;5;241m=\u001b[39m PongPreprocessingWrapper(\u001b[43menv\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from collections import deque\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    # Convert to grayscale and resize to 84x84\n",
    "    img = Image.fromarray(frame)\n",
    "    img = img.convert('L').resize((84, 84))\n",
    "    return np.array(img)\n",
    "\n",
    "# Wrapping the environment to include our preprocessing\n",
    "class PongPreprocessingWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(PongPreprocessingWrapper, self).__init__(env)\n",
    "        self.frames = deque(maxlen=4)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 4), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "        if len(self.frames) < 4:\n",
    "            for _ in range(4):\n",
    "                self.frames.append(processed_frame)\n",
    "        self.frames.append(processed_frame)\n",
    "        return np.stack(self.frames, axis=2)\n",
    "\n",
    "# Applying the preprocessing wrapper\n",
    "env = PongPreprocessingWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46545392",
   "metadata": {},
   "source": [
    "## Step 6: Training the DQN Agent\n",
    "In this step, we will train the DQN agent on the Pong environment. If pre-trained weights are available, we can load them; otherwise, we'll start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27790f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the DQN agent\n",
    "start_training = False  # Change to True to start training\n",
    "if start_training:\n",
    "    dqn.fit(env, nb_steps=1000000, log_interval=10000)\n",
    "    # Save the final weights\n",
    "    dqn.save_weights('dqn_pong_weights.h5f', overwrite=True)\n",
    "else:\n",
    "    # Load pre-trained weights if available\n",
    "    dqn.load_weights('dqn_pong_weights.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdfd592",
   "metadata": {},
   "source": [
    "## Step 7: Testing the Trained Agent\n",
    "After training, we can test the agent's performance by running it on a few episodes and visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335820a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the agent's performance after training\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d362a",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "After training, the DQN agent should have learned how to play Pong efficiently. The quality of play can be observed during the test episodes. If the agent is playing well, it should consistently win against the opponent in the game.\n",
    "\n",
    "Here are some important metrics to observe:\n",
    "- **Total rewards**: The reward the agent accumulates over an episode helps evaluate performance.\n",
    "- **Win rate**: The number of episodes the agent wins divided by the total number of episodes.\n",
    "- **Policy improvement**: Observe how the agent's strategy evolves over time.\n",
    "\n",
    "If the training performance is not satisfactory, consider the following improvements:\n",
    "\n",
    "- **Extend the number of training steps**: Training for a larger number of steps can help the agent to explore more and learn better.\n",
    "- **Fine-tune hyperparameters**: Adjust learning rate, memory size, exploration policy parameters, etc., for improved results.\n",
    "- **Improve preprocessing of frames**: Ensuring that the preprocessing of frames is optimal can lead to better state representations.\n",
    "- **Use advanced algorithms**: Consider using Double DQN, Dueling DQN, or Prioritized Experience Replay for potentially better performance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we trained a reinforcement learning agent using the Deep Q-Learning (DQN) algorithm to play Pong. The process involved:\n",
    "\n",
    "1. **Setting up the environment**: Utilizing OpenAI Gym to create the Pong environment.\n",
    "2. **Designing a CNN-based neural network**: Building a neural network suitable for processing frames from the game and predicting Q-values.\n",
    "3. **Configuring the DQN agent**: Setting up the DQN agent with the neural network, policy, and memory.\n",
    "4. **Preprocessing frames**: Converting frames to grayscale, resizing, and stacking consecutive frames to provide temporal context.\n",
    "5. **Training the agent**: Training the agent over multiple episodes and updating its policy using the experience gained.\n",
    "6. **Testing performance**: Evaluating the agent's performance after training to assess how well it learned to play the game.\n",
    "\n",
    "This approach demonstrates how neural networks can be integrated with reinforcement learning techniques to create agents capable of learning and mastering complex tasks such as playing Pong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d9a50",
   "metadata": {},
   "source": [
    "# Using `stable-baselines3`\n",
    "\n",
    "Is an alternative library which provide similar functionality and might be more compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d38c7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "## 1: Download the ROMs\n",
    "1. **Visit the Atari ROMs repository**:\n",
    "   - The repository can be found [here](https://github.com/openai/atari-py#roms).\n",
    "  \n",
    "\n",
    "2. **Download the `ROMS.zip` file**:\n",
    "   - Follow the instructions provided in the repository to obtain the `ROMS.zip` file or switch [here](https://www.atarimania.com/rom_collection_archive_atari_2600_roms.html).\n",
    "\n",
    "3. **Extract the `ROMS.zip` to a directory of your choice**:\n",
    "   - Use your preferred file extraction tool to unzip the `ROMS.zip` file to a directory on your computer.\n",
    "\n",
    "## 2: Install the ROMs using `ale-import-roms`\n",
    "\n",
    "1. **Use the `ale-import-roms` utility**:\n",
    "   - This utility will load the ROMs into the Arcade Learning Environment (ALE).\n",
    "\n",
    "2. **Run the following command in your terminal**:\n",
    "   ```bash\n",
    "   ale-import-roms <path-to-rom-directory>```\n",
    "   \n",
    "Replace <path-to-rom-directory> with the actual path to the directory where you extracted the ROMs.\n",
    "Example: \n",
    "    ```bash \n",
    "    ale-import-roms /home/user/Downloads/ROMS```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04f3bd",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the Environment\n",
    "First, ensure you have all necessary libraries installed. If not, you can install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0e5a6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (1.24.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.8.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gym[atari]) (0.8.1)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3) (0.29.1)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3) (2.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3) (1.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stable-baselines3) (3.7.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anna.androvitsanea\\appdata\\roaming\\python\\python310\\site-packages (from ale-py~=0.8.0->gym[atari]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2023.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->stable-baselines3) (2022.7.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anna.androvitsanea\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\anna.androvitsanea\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install gym[atari] stable-baselines3 opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c9d06",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Initialize Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36348c2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"Pong\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Pong\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Pong\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CheckpointCallback\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Set up the Pong environment with Atari wrappers\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPongNoFrameskip-v4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m env \u001b[38;5;241m=\u001b[39m AtariWrapper(env)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Verify action space\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_creator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[0;32m    645\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ale_py\\env\\gym.py:155\u001b[0m, in \u001b[0;36mAtariEnv.__init__\u001b[1;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, max_num_frames_per_episode, render_mode)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetBool(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Seed + Load\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetLegalActionSet()\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m full_action_space\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgetMinimalActionSet()\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_set))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ale_py\\env\\gym.py:206\u001b[0m, in \u001b[0;36mAtariEnv.seed\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39msetInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed2\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32))\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mre Unable to find the game \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Note: Gym no longer distributes ROMs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you own a license to use the necessary ROMs for research purposes you can download them \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia `pip install gym[accept-rom-license]`. Otherwise, you should try importing \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvia the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis unsupported. To check if this is the case try providing the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mloadROM(\u001b[38;5;28mgetattr\u001b[39m(roms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_game_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mError\u001b[0m: We're Unable to find the game \"Pong\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Pong\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Pong\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Set up the Pong environment with Atari wrappers\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = AtariWrapper(env)\n",
    "\n",
    "# Verify action space\n",
    "nb_actions = env.action_space.n\n",
    "print(f\"Number of actions: {nb_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65a9c6",
   "metadata": {},
   "source": [
    "## Step 3: Build and Configure the DQN Agent\n",
    "`stable-baselines3` allows building and configuring DQN agents easily with a single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53a0030c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m policy_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m      3\u001b[0m     net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m512\u001b[39m],\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create the DQN agent\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 9\u001b[0m     \u001b[43menv\u001b[49m,\n\u001b[0;32m     10\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m,\n\u001b[0;32m     11\u001b[0m     buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[0;32m     12\u001b[0m     learning_starts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m     14\u001b[0m     tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     15\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m     16\u001b[0m     train_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     17\u001b[0m     target_update_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     18\u001b[0m     exploration_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m     19\u001b[0m     exploration_final_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.02\u001b[39m,\n\u001b[0;32m     20\u001b[0m     policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs,\n\u001b[0;32m     21\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[512],\n",
    ")\n",
    "\n",
    "# Create the DQN agent\n",
    "model = DQN(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=0.0001,\n",
    "    buffer_size=10000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b65264",
   "metadata": {},
   "source": [
    "## Step 4: Training the DQN Agent\n",
    "We'll set up training with a callback to save model checkpoints periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0aca56a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m start_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Change to True to start training\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_training:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, callback\u001b[38;5;241m=\u001b[39mcheckpoint_callback)\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdqn_pong_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'learn'"
     ]
    }
   ],
   "source": [
    "# Define checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./logs/', name_prefix='dqn_pong_model')\n",
    "\n",
    "# Train the model\n",
    "start_training = True  # Change to True to start training\n",
    "if start_training:\n",
    "    model.learn(total_timesteps=100000, callback=checkpoint_callback)\n",
    "    model.save(\"dqn_pong_model\")\n",
    "else:\n",
    "    model.load(\"dqn_pong_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae29ab6",
   "metadata": {},
   "source": [
    "## Step 5: Testing the Trained Agent\n",
    "After training, we will test the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent's performance after training\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = AtariWrapper(env)\n",
    "obs = env.reset()\n",
    "for _ in range(5000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b858979",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "After training, the DQN agent should have learned how to play Pong efficiently. The quality of play can be observed during the test episodes. If the agent is playing well, it should consistently win against the opponent in the game.\n",
    "\n",
    "Here are some important metrics to observe:\n",
    "\n",
    "- **Total rewards**: The reward the agent accumulates over an episode helps evaluate performance.\n",
    "- **Win rate**: The number of episodes the agent wins divided by the total number of episodes.\n",
    "- **Policy improvement**: Observe how the agent's strategy evolves over time.\n",
    "\n",
    "If the training performance is not satisfactory, consider the following improvements:\n",
    "\n",
    "- **Extend the number of training steps**: Training for a larger number of steps can help the agent to explore more and learn better.\n",
    "- **Fine-tune hyperparameters**: Adjust learning rate, memory size, exploration policy parameters, etc., for improved results.\n",
    "- **Improve preprocessing of frames**: Ensuring that the preprocessing of frames is optimal can lead to better state representations.\n",
    "- **Use advanced algorithms**: Consider using Double DQN, Dueling DQN, or Prioritized Experience Replay for potentially better performance.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we trained a reinforcement learning agent using the Deep Q-Learning (DQN) algorithm to play Pong. The process involved:\n",
    "\n",
    "1. **Setting up the environment**: Utilizing OpenAI Gym to create the Pong environment.\n",
    "2. **Designing a CNN-based neural network**: Building a neural network suitable for processing frames from the game and predicting Q-values.\n",
    "3. **Configuring the DQN agent**: Setting up the DQN agent with the neural network, policy, and memory.\n",
    "4. **Preprocessing frames**: Converting frames to grayscale, resizing, and stacking consecutive frames to provide temporal context.\n",
    "5. **Training the agent**: Training the agent over multiple episodes and updating its policy using the experience gained.\n",
    "6. **Testing performance**: Evaluating the agent's performance after training to assess how well it learned to play the game.\n",
    "\n",
    "This approach demonstrates how neural networks can be integrated with reinforcement learning techniques to create agents capable of learning and mastering complex tasks such as playing Pong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c9931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
